{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec805085-fbb0-4c9c-b550-377baa261f57",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "\n",
    "# Interactive Image Classification with Jupyter Notebooks in Red Hat OpenShift AI\n",
    "\n",
    "\n",
    "\n",
    "**Date**: Nov 14, 2024  \n",
    "\n",
    "**Authors**: Alex Krikos & Ramakrishna Yekulla  \n",
    "\n",
    "\n",
    "\n",
    "### Related Topics:\n",
    "\n",
    "\n",
    "\n",
    "- Developer Tools\n",
    "\n",
    "- Event-Driven\n",
    "\n",
    "\n",
    "\n",
    "### Related Products:\n",
    "\n",
    "\n",
    "\n",
    "- Red Hat OpenShift\n",
    "\n",
    "- Red Hat OpenShift AI\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Jupyter Notebooks within Red Hat OpenShift AI to interactively classify images of cats and dogs. We leverage TensorFlow and ipywidgets to simulate real-time data streaming and visualization.\n",
    "\n",
    "\n",
    "\n",
    "## Prerequisites:\n",
    "\n",
    "\n",
    "\n",
    "- Access to Red Hat Developer Sandbox.\n",
    "\n",
    "- An active Red Hat OpenShift cluster.\n",
    "\n",
    "- Basic knowledge of Python programming.\n",
    "\n",
    "- A GitHub account for accessing code repositories.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# JupyterLab Setup and Dataset Preparation\n",
    "\n",
    "## Step 1: Launch JupyterLab\n",
    "\n",
    "1. **Navigate to the OpenShift AI Dashboard:**\n",
    "   - Visit the **OpenShift AI Dashboard** within the OpenShift AI platform.\n",
    "\n",
    "2. **Access Data Science Projects:**\n",
    "   - Go to **\"Data Science Projects\"**.\n",
    "   - Select your project from the list.\n",
    "\n",
    "3. **Create a New Workbench:**\n",
    "   - Navigate to the **\"Workbenches\"** tab.\n",
    "   - Click **\"Create Workbench\"**.\n",
    "   \n",
    "   - **Configuration:**\n",
    "     - **Name**: Set a suitable name for your workbench.\n",
    "     - **Notebook Image**: Choose **TensorFlow**.\n",
    "     - **Deployment Size**: Select **\"Medium\"**.\n",
    "     - **Cluster Storage**: Allocate **20Gi**.\n",
    "\n",
    "   - Click **\"Create Workbench\"** and wait for the status to show **\"Running\"**.\n",
    "\n",
    "## Step 2: Obtain and Prepare the Dataset\n",
    "\n",
    "### Purpose:\n",
    "- Install the Kaggle library and configure the Kaggle API key for downloading datasets programmatically.\n",
    "\n",
    "### Why:\n",
    "- Using the Kaggle API key enables direct access to large datasets for machine learning, enhancing data acquisition efficiency.\n",
    "\n",
    "### Steps:\n",
    "### Configure Kaggle API Key:\n",
    "\n",
    "**Steps to configure:**\n",
    "\n",
    "1. **Download your Kaggle API key:** \n",
    "   - Access your Kaggle account settings and download the `kaggle.json` file.\n",
    "\n",
    "2. **Setup the API Key on your Workbench:**\n",
    "   - Place the `kaggle.json` file in the `.kaggle/` directory within your home directory on the workbench. Here are the commands:\n",
    "\n",
    "     ```bash\n",
    "     mkdir -p ~/.kaggle\n",
    "     mv kaggle.json ~/.kaggle/\n",
    "     chmod 600 ~/.kaggle/kaggle.json\n",
    "     ```\n",
    "\n",
    "**Security Note**: \n",
    "- This setup ensures that your API key is secure. The `chmod 600` command sets the file permissions so that only the owner can read or write the file, enhancing security.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77b861-1826-43ba-b7b8-6aa23c2a1a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "import os\n",
    "os.makedirs('~/.kaggle', exist_ok=True)\n",
    "!echo '{\"username\":\"YOUR_USERNAME\",\"key\":\"YOUR_API_KEY\"}' > ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d salader/dogs-vs-cats --force\n",
    "!unzip -oq dogs-vs-cats.zip -d ./data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c25b1b-6456-486a-8523-5159f297d7c4",
   "metadata": {},
   "source": [
    "## Step 3: Build and Train the Model\n",
    "\n",
    "### **Why This Step Matters**:\n",
    "- **Monitoring Performance**: Observing training logs allows us to assess if the model is effectively learning from the data.\n",
    "- **Identifying Problems**: We can spot signs of overfitting or underfitting, which are critical for model optimization.\n",
    "- **Guiding Adjustments**: Insights from logs help in making informed decisions about tweaking model parameters or architecture.\n",
    "\n",
    "### **Process**:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Setup data generators to preprocess images, which includes scaling, augmenting, or normalizing data for better training.\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - Design the CNN with layers like convolutional, pooling, and fully connected layers to learn image features.\n",
    "\n",
    "3. **Model Training**:\n",
    "   - Compile the model with an optimizer, loss function, and metrics like accuracy.\n",
    "   - Train the model using the prepared dataset, adjusting weights through backpropagation to minimize loss.\n",
    "\n",
    "#### **Dataset Overview**:\n",
    "\n",
    "- **Output**: `\"Found 20000 images belonging to 2 classes.\"`\n",
    "  - **Explanation**: Your dataset comprises 20,000 images split into two classes, likely representing 'cats' and 'dogs'.\n",
    "  - **Significance**: This ensures you have enough data to train on and that the classes are balanced, which is crucial for model fairness.\n",
    "\n",
    "### **Training Metrics**:\n",
    "\n",
    "- **Epochs**: The model goes through the entire dataset 5 times, allowing it multiple chances to learn.\n",
    "- **Steps per Epoch**: Each epoch is divided into 50 steps, where each step processes a batch of data.\n",
    "- **Accuracy**: Indicates the percentage of correct classifications, with 0.5723 at the first epoch suggesting the models initial performance.\n",
    "- **Loss**: A measure of prediction error, where a decrease over time signifies improvement.\n",
    "\n",
    "#### **Epoch by Epoch Analysis**:\n",
    "\n",
    "- **Epoch 1**: Begins with a high loss (0.7998) and an accuracy of 57.23%, marking the baseline performance.\n",
    "- **Epochs 2-5**: Demonstrate slight progress in accuracy and a trend of reducing loss, with some normal fluctuations.\n",
    "\n",
    "#### **Key Insights**:\n",
    "\n",
    "- **Model Learning**: A decreasing loss shows the model is absorbing information, but the improvement rate hints at potential for further optimization.\n",
    "- **Overfitting/Underfitting**: Although not directly visible, understanding these concepts is vital for model validation.\n",
    "\n",
    "#### **Strategies for Enhancement**:\n",
    "\n",
    "- **Model Architecture**: Consider different or additional layers, or altering existing layer parameters.\n",
    "- **Learning Rate**: Tuning this can lead to better convergence and learning stability.\n",
    "- **Data Augmentation**: Enhances the datasets variety, helping the model generalize better from the training data.\n",
    "\n",
    "#### **Continuous Monitoring**:\n",
    "\n",
    "- Regularly check these metrics to make timely adjustments in your training strategy.\n",
    "\n",
    "**This step-by-step analysis equips learners with the tools to interpret training outputs, troubleshoot issues like overfitting or underfitting, and refine their models to achieve better performance.**\n",
    "**This summary and analysis from training logs are essential for learners to understand model performance dynamics, diagnose issues, and strategize improvements in machine learning projects.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee90218-ef51-478f-aee3-26b456a4825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Setup data generators with a smaller batch size for faster execution\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './data/train',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=8,  # Further reduced batch size\n",
    "        class_mode='binary')\n",
    "\n",
    "# Define the CNN with an explicit Input layer\n",
    "model = Sequential([\n",
    "    Input(shape=(150, 150, 3)),  # Explicit input shape definition\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),  # Reduced the number of neurons for faster processing\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Reduce the number of epochs to speed up the training\n",
    "# Reduced epochs and steps per epoch\n",
    "model.fit(train_generator, steps_per_epoch=50, epochs=5)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913bd3e-3909-48f5-b374-4d90d1f590cc",
   "metadata": {},
   "source": [
    "### Step 4: Interactive Real-Time Data Streaming and Visualization\n",
    "\n",
    "**Purpose**: Simulate real-time data interaction and demonstrate how AI can be used interactively in a simplified context. We leverage TensorFlow and ipywidgets to simulate real-time data streaming and visualization.\n",
    "\n",
    "**Process**:\n",
    "1. **Create Interactive Dropdown Menus**: Implement dropdown menus to allow users to select and visually interact with predictions on cat or dog images.\n",
    "2. **Utilize Widgets for Simulation**: Use Jupyter Notebook widgets to simulate real-time data streaming. This approach is particularly useful for educational and demonstration purposes, showing how data can be dynamically processed and visualized in an interactive environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812179c9-8670-4b0f-99e8-053fd6d53db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for handling images\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(file_path):\n",
    "    img = image.load_img(file_path, target_size=(150, 150))\n",
    "    img_tensor = image.img_to_array(img)                    # Convert image to array\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)         # Adjust shape for model input\n",
    "    img_tensor /= 255.0                                    # Normalize the image\n",
    "    return img_tensor\n",
    "\n",
    "# Function to display predictions\n",
    "def predict_and_visualize(model, file_path):\n",
    "    img_tensor = load_and_preprocess_image(file_path)\n",
    "    prediction = model.predict(img_tensor)\n",
    "    plt.imshow(img_tensor[0])\n",
    "    plt.title(f'Prediction: {\"Dog\" if prediction[0][0] > 0.5 else \"Cat\"}')\n",
    "    plt.show()\n",
    "\n",
    "# Function to select a random image and make a prediction\n",
    "def predict_random_image(model, directory):\n",
    "    if os.path.exists(directory):\n",
    "        random_image = random.choice(os.listdir(directory))\n",
    "        full_path = os.path.join(directory, random_image)\n",
    "        predict_and_visualize(model, full_path)\n",
    "    else:\n",
    "        print(\"Directory not found:\", directory)\n",
    "        \n",
    "# Update these paths according to your actual directory structure\n",
    "cat_path = './data/test/cats/'\n",
    "dog_path = './data/test/dogs/'\n",
    "\n",
    "# Assuming 'model' is your trained model, and 'cat_path' and 'dog_path' are defined\n",
    "# Example usage\n",
    "predict_random_image(model, cat_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f29890-6618-4947-b12e-7093f99cc53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_random_image(model, dog_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d990f83-daa0-473f-9ba6-c68f4620805d",
   "metadata": {},
   "source": [
    "### Step 5: Testing the Dataset with Random Image Prediction\n",
    "\n",
    "**Purpose**: This step is crucial for evaluating the trained model's performance by testing it on unseen data in a real-world-like scenario. By randomly selecting images and predicting their classes, we can visually assess the model's accuracy and reliability.\n",
    "\n",
    "**Process**:\n",
    "1. **Random Image Selection**: The function `predict_random_image` takes a directory path as input and checks if the path exists. It then randomly selects an image from this directory, ensuring that each test is unbiased and represents a realistic use case.\n",
    "2. **Image Loading and Preprocessing**: The randomly chosen image file is then loaded and preprocessed to match the input format expected by the model. This involves resizing the image and normalizing its pixel values.\n",
    "3. **Model Prediction**: The preprocessed image tensor is fed into the model to predict whether the image is of a 'Cat' or 'Dog'. This step directly utilizes the neural network to interpret the image data.\n",
    "4. **Visualization**: The image along with its predicted class is displayed. This visual feedback is crucial for understanding the model's decision-making process and immediately seeing the result of the prediction.\n",
    "5. **Interactive Testing**: By running the function with different directories (e.g., cats and dogs), users can interactively test how the model performs across varied inputs, making this a dynamic tool for demonstration and educational purposes.\n",
    "\n",
    "**Why This Step**:\n",
    "Testing the model with a random selection of images simulates how the model might perform in a production environment where inputs are not predetermined. It helps in identifying potential biases, underfitting, or overfitting issues in the model. Additionally, visual feedback from test predictions is an excellent way to demonstrate the model's capabilities to a non-technical audience, making complex machine learning concepts more accessible and understandable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d648be1-7f9e-40ea-8f26-4e8bcb82198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def predict_random_image(model, directory):\n",
    "    if os.path.exists(directory):\n",
    "        random_image = random.choice(os.listdir(directory))\n",
    "        full_path = os.path.join(directory, random_image)\n",
    "        img_tensor = load_and_preprocess_image(full_path)\n",
    "        prediction = model.predict(img_tensor)\n",
    "        plt.imshow(img_tensor[0])\n",
    "        plt.title(f'Prediction: {\"Dog\" if prediction[0][0] > 0.5 else \"Cat\"}')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "predict_random_image(model, './data/test/cats/')\n",
    "predict_random_image(model, './data/test/dogs/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988fa601-507d-44dc-9f04-e5d7c6614c7f",
   "metadata": {},
   "source": [
    "### Step 6: Interactive Real-Time Image Prediction with Widgets\n",
    "\n",
    "**Purpose**: This step integrates interactive web widgets to provide a user-friendly interface for real-time image prediction, showcasing how TensorFlow and Jupyter Notebook widgets can be used to enhance the interactivity and accessibility of AI applications.\n",
    "\n",
    "**Process**:\n",
    "1. **Import Necessary Libraries**:\n",
    "   - Libraries such as `ipywidgets` for interactive controls, `matplotlib.pyplot` for visualization, `os` for operating system interface, `random` for randomness, `tensorflow.keras.preprocessing.image` for image handling, and `numpy` for numerical operations are essential for the functionality of this code.\n",
    "2. **Define Image Loading and Preprocessing Function**:\n",
    "   - `load_and_preprocess_image`: Loads and processes images to match the input specifications of the neural network, including resizing to 150x150 pixels and normalizing pixel values to the range [0,1].\n",
    "3. **Define Prediction and Visualization Function**:\n",
    "   - `predict_and_visualize`: Utilizes the trained model to classify the image as either 'Cat' or 'Dog' and visualizes the image alongside its classification, providing immediate visual feedback on the prediction outcome.\n",
    "4. **Setup Interactive Widgets for User Input and Display**:\n",
    "   - A button widget labeled \"Inceptial\" is used to initiate the prediction of a randomly selected image from either the cats or dogs directory.\n",
    "   - An output widget displays the results and handles any necessary clearing of previous outputs for clarity.\n",
    "5. **Implement Random Image Prediction Functionality**:\n",
    "   - `predict_random_image`: Collects all image paths from specified directories, randomly selects one, and performs prediction and visualization. This simulates a realistic scenario where the model might be used in a production environment to classify new, unseen images.\n",
    "6. **Widget Interaction Setup**:\n",
    "   - Link the button to trigger the random image prediction function, allowing users to interactively test the modelâ€™s performance on various images with a single click.\n",
    "\n",
    "**Why This Step**:\n",
    "- **Enhancing User Engagement**: Using interactive widgets makes the application more accessible and engaging for users, who can actively participate in the testing process.\n",
    "- **Demonstrating Model Capabilities**: This setup provides a practical demonstration of the model's capabilities in a dynamic, real-world application, allowing for the assessment of its robustness and accuracy.\n",
    "- **Educational Tool**: It serves as an excellent educational tool, helping users understand machine learning concepts through direct interaction and immediate feedback.\n",
    "\n",
    "**Output**:\n",
    "- The interactive session will display images with their predicted labels in real-time as the user clicks the \"Inceptial\" button. This dynamic interaction helps in understanding how well the model performs across a random set of images and provides insights into potential improvements for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962a5229-f012-421d-96ef-cf63c79b294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'model' is your trained model\n",
    "def load_and_preprocess_image(file_path):\n",
    "    img = image.load_img(file_path, target_size=(150, 150))\n",
    "    img_tensor = image.img_to_array(img)\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
    "    img_tensor /= 255.0\n",
    "    return img_tensor\n",
    "\n",
    "def predict_and_visualize(model, file_path):\n",
    "    img_tensor = load_and_preprocess_image(file_path)\n",
    "    prediction = model.predict(img_tensor)\n",
    "    plt.imshow(img_tensor[0])\n",
    "    plt.title(f'Prediction: {\"Dog\" if prediction[0][0] > 0.5 else \"Cat\"}')\n",
    "    plt.show()\n",
    "\n",
    "# Path settings\n",
    "base_path = '/opt/app-root/src/data/test/'\n",
    "animal_types = {'Cats': 'cats/', 'Dogs': 'dogs/'}\n",
    "\n",
    "# Widgets\n",
    "predict_button = widgets.Button(description=\"Inceptial\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def predict_random_image(b):\n",
    "    # Gather all images from both categories\n",
    "    all_images = []\n",
    "    for animal, folder in animal_types.items():\n",
    "        full_path = os.path.join(base_path, folder)\n",
    "        all_images.extend([os.path.join(full_path, file) for file in os.listdir(full_path)])\n",
    "    \n",
    "    if all_images:\n",
    "        random_image_path = random.choice(all_images)\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            predict_and_visualize(model, random_image_path)\n",
    "    else:\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"No images found.\")\n",
    "\n",
    "# Link the button to the random image prediction function\n",
    "predict_button.on_click(predict_random_image)\n",
    "\n",
    "# Display widgets\n",
    "display(predict_button, output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a9c090-e4bd-4c6b-96b9-67a708412c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
